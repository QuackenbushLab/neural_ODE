ODENet(
  (net): Sequential(
    (activation_0): Softsign()
    (linear_1): Linear(in_features=690, out_features=150, bias=True)
    (activation_1): Softsign()
    (linear_out): Linear(in_features=150, out_features=690, bias=True)
  )
  (net2): Sequential(
    (linear_out): Linear(in_features=690, out_features=690, bias=True)
    (activation_0): Sigmoid()
  )
)


    def forward(self, t, y):
        grad1 = self.net(y)
        grad2 = self.net2(y)
        if self.log_scale == "log":
            final = torch.exp(grad1-y) + grad2
        else:
           final = grad2*(grad1 - y)     
        return(final) 
