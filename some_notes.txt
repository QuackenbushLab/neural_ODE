

Questions for Rebekka (early august 2020): 
0) updates:
    a) not too much progress
    b) found resources (thesis that I'm going through)
    c) found various code bases
    d) just to clarify:
        I) parameters are the only things we are learning right?
        II) we PROVIDE the functional form of the ODE to the algo?  
   
1) what can neural ODEs achieve:
    a) need the underlying mouse gene network
    b) can only get parameters
    c) or maybe functional form of ODE, but not the nature of the network

2) plans for upcoming 2 weeks: 
    a) no fixed code base
    b) any pytorch resources?
    c) any go-to person in the lab for queries/updates? 
    d) any need to write up anything?


August 21 2020: 
0) Theoritical discussion: 
    a) no functional forms go into NeuralODE algorithm.
    b) two parts, NN + ODESolver, where the NN *is* the derivative functional
    c) Can do functional fitting to retrieve parameters
    
1) Updates: 
    a) a lot of progress made, have a solid pipeline set-up and currently producing results
    b) Sweden story; + IH modifications (pytorch resources) 
    c) The code base can do a lot, various forms of sampling as well as functional fitting, but some parts are confusing
    d) Parameters, and results obtained so far
    e) Need to send an email to Sweden regarding various questions; any IP issues/ anything to be wary of? 
    f) After getting clarification from them, then perhaps can start designing experiments, and tuning the network??  
    e) Need to get this set up and running on the cloud (AWS vs FASRC)

2) Presentation: 
    a) have an outline so far (share outline)
    b) do you want to take a look at the presentation before I give the talk? If so does Monday work? 
    c) any need to share with JQ beforehand? 

3) Future steps: 
    a) is it ok to continue the project (seems like we're closing in on the interesting part + mention paper)?
    b) will RB talk to JQ or should I (preference for RB)? 


9/4/2020 notes: 
1)  Speed-tests:
    a) 150 genes, 6 samples (5T+1V), 25 epochs, local = 54 mins, MSE = 0.15
    b) 150 genes, 6 samples (5T+1V), 25 epochs, AWS cpu = 10 mins, MSE = 0.15
    c) 150 genes, 6 samples (5T+1V), 25 epochs, AWS gpu = 49 mins, MSE = 0.15
seems like CPU on AWS is best option so far..

9/8/2020 notes: 
1) changing neural network #percetrons
    a) will try 2 hidden layers of 300 perceptrons each 
        - (input = 150 genes, output = 150 genes)
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.068
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.064
        - 150 genes, 6 samples (5TAll+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.1
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS cpu = 3.6 hrs, MSE = 0.008
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.008
    b) JQ feedback (lab meeting): 
        - maybe move forward with actual YEAST time-series data (e.g. cell-cycle) first
        - what are these 150 genes in the Yeast GRN (from Bhuva 2019)?

9/9/2020 notes: 
1) Email to Chalmers team
    - Undestand explicit time functionality
        - works with type = single
        - ran on AWS gpu, 30 genes, 6 samples, 3 epochs
    - Understand various batching methods 
    - Understand train/predict based on time = 0 vs all times
2) Check Bhuva 2019 (temporal data)
    - no mention of time-series data (mention found in repository; WE made the changes)
    - they developed the DIFFERENTIAL CO-EXP method using a yeast network, and then applied to TCGA
        - i.e. organism agnostic
    - Marbach D, Schaffter T, Mattiussi C, Floreano D. Generating realistic in silico
gene networks for performance assessment of reverse engineering
methods. J Comput Biol. 2009;16:229â€“39.
    - http://genome-www.stanford.edu/cellcycle/


9/10/2020 notes:
1) Experimenting with explicit time = T and batch_type = single
    - seems to give overall better MSE values when using 30 genes and 6 samples
    - only runs on CPU not GPU (why?)
    - scaling up, let's see what happens:
        - 150 genes, 6 samples (explct = T, batch = single), 
            15 epochs, 300 perceptron-layers, 
            AWS cpu = 2.08 hrs, MSE = 0.007
        - 150 genes, 8 samples (explct = T, batch = single), 
            15 epochs, 300 perceptron-layers, 
            AWS cpu = 1.89 hrs, MSE = 0.01


9/11/2020 notes: 
1) Experimenting with explicit time = T and batch_type = single on t2.2xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = x hrs, MSE = x  
        - (TOOK TOO LONG, no improvement over p2.xlarge)
        - could consider just using this if we only do CPU since it's cheaper
2) Experimenting with explicit time = T and batch_type = single on c5d.4xlarge AWS (CPU only)
    - Instance type supposed to be compute-optimized (16 vcpu + 32GB RAM)
    - 150 genes, 8 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = 1.47 hrs, MSE = 0.007
    - 150 genes, 6 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = 1.5 hrs, MSE = 0.005 

9/13/2020 notes:
1) Experimenting with explicit time = T and batch_type = single on c5d.4xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.15), batch_size = 2
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.15), batch_size = 4
2) Experimenting with explicit time = T and batch_type = single on c5.9xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.2), batch_size = 2
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.2), batch_size = 4
3) Have emailed Chalmers team for time to talk about code; waiting...

9/14/202 notes:
1) Investigate explicit time, vis, and batch methods on own
    - batch methods = got a good understanding now; we should probably stick to pairwise train-test (i.e "single")
    - explicit time = from the looks of it, doesn't seem like it affects predictions much (need to confirm)!
    - Now running few experiments to see if effect of explicit time is substantial
    - MSE still around 0.012, but runtime is very fast (<30 mins). 
    - GOOD to keep batch_size = 2 and explicit = False!
    - Let's increase epochs to 25
        - MSE goes down to 0.007
        - So should stick to 25 epochs!
    - Now let's test again on GPU (explicit = F, batch = single)
        - seems like explicit = F, batch = single and batch_size = 2 is optimal 
2) Scope to optimize:
    - good initialization (RB to send)
    - domain knowledge into NN (ask Anuraag about this)
    - graph neural network (GNN)
4) Check for real yeast data
    - yeast/human gene-communities
    - how does it scale? (base how much to scale on the structure of the real yeast data)
    - should we stick to yeast??

9/15/202 notes:
1) Start plotting predicted dynamics
    - made progress in terms of this. got plotting function to work.
    - testing, works fine and plots make sense!
    - tested with 0 noise dataset:
        - looks very good! approximations are very accurate. 

9/16/202 notes:
1) Trying to increase epochs so that training error is essentially 0 on 0 noise dataset.
    - will run on 30 genes, 6 sample, 0 noise dataset for 40 epochs
        - predictions not near-perfect yet. Scaling up epochs and perceptrons  
            - why does training_loss fluctuate, and not monotonically decrease? (Stochastic GD!)
            - think about best_training_model vs best_validation_model
        - changed code so that we also plot training loss over epochs
    - running on 0 noise dataset but with n_val = 0, i.e. val_split = 0
        - we see that we have near-perfect training fit!
    - can use c5.18xlarge as next step ($3.06 per hour), i.e. when scaling up to more genes with 0noise. 

9/17/2020 notes:
1) Meeting with RB. Potential things to try: 
	- larger batch size
	- smaller learning rate decay
	- start with best model already
	- next step scale up to more than 30 genes

9/27/2020 notes:
0) Changed plotting function to plot all 30 genes; also keeping track of training error now
1) Batch_size experiments:
    - When batch_size = 100% full (no stochastic GD), runtime = 4.86 hrs
    - When batch_size = 50%, runtime ~ 3.75
    - When batch_size = 33%, runtime ~ 2.75 hrs
    - When batch_size = 16% , runtime = 2.33 hrs
    - When batch_size = 3% (2) , runtime = 2.24 hrs
    - When batch_size = 1.5% (1; fully stochastic) , runtime = 2.31 hrs
    - In *general*, also saw better performance with smaller batches
2) Things to explore:
    - Performance-wise: different initial learning rates, different network structure, etc:
        -  NeuralODE authors: "Avoid non-smooth non-linearities such as ReLU and LeakyReLU.
            Prefer non-linearities with a theoretically unique adjoint/gradient such as Softplus."
    - Runtime-wise: GPU capacity: 
        - Chalmers authors: "Due to the way the ODE solver was implemented we were only able
         to parallelize computations on the GPU if they were for time steps of the same length.
          Further research should be conducted on the topic of extending the ODE solvers to also
           be able to run different time steps in parallel to speed up training."
        - Need to look more closely into code about how "cude", "cuda:0" are used, and if available
            devices are being used efficiently. 
        - running experiment on AWS p3.2xlarge ($3.02 per hour!):
            - init_lr = 0.001, dec_lr = False, batch size = 54 (for parallelization)
            - MSE = NA, runtime = way too long (40 mins for first epoch!)  

9/28/2020 notes: 
0) Booted a new AWS instance (c5.9xlarge)
1) Question for RB:
    - what is weight-decay? should we use it now? 
2) Experiments with init_lr = 0.001, dec_lr = False:
    - should try out different batch sizes here as well. 

10/04/2020 notes:
0) Booted a new AWS instance (c5.18xlarge)
    - now reporting time and MSE at epochs of interest (25, 40, 50, 80)
1) Trying the non-stochastic (batch_size = 27) upto 160 epochs
    - trying upto 160 epochs with init_lr = 0.005 and 0.001
        - 0.005 better constant lr, but we see that we keep maxing out around ~10^-6
2) Try other things
    - Increased to 500 perceptrons for bs = 27 (non-stochastic)
    - Trying both xavier_uniform and orthogonal initialization schemes
        -orthogonal much better to start with 

10/05/2020 notes:
0) changed file structure to reduce cluttering
1) changed training code:
    - now saving the best performing model instead of final model
    - can load in pre-trained model from .pt file
2) Will now load in best-performing model from before (3E-06), and try to improve using slow lr and decay



