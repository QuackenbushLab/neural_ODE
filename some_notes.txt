Questions for Rebekka (early august 2020): 
0) updates:
    a) not too much progress
    b) found resources (thesis that I'm going through)
    c) found various code bases
    d) just to clarify:
        I) parameters are the only things we are learning right?
        II) we PROVIDE the functional form of the ODE to the algo?  
   
1) what can neural ODEs achieve:
    a) need the underlying mouse gene network
    b) can only get parameters
    c) or maybe functional form of ODE, but not the nature of the network

2) plans for upcoming 2 weeks: 
    a) no fixed code base
    b) any pytorch resources?
    c) any go-to person in the lab for queries/updates? 
    d) any need to write up anything?


August 21 2020: 
0) Theoritical discussion: 
    a) no functional forms go into NeuralODE algorithm.
    b) two parts, NN + ODESolver, where the NN *is* the derivative functional
    c) Can do functional fitting to retrieve parameters
    
1) Updates: 
    a) a lot of progress made, have a solid pipeline set-up and currently producing results
    b) Sweden story; + IH modifications (pytorch resources) 
    c) The code base can do a lot, various forms of sampling as well as functional fitting, but some parts are confusing
    d) Parameters, and results obtained so far
    e) Need to send an email to Sweden regarding various questions; any IP issues/ anything to be wary of? 
    f) After getting clarification from them, then perhaps can start designing experiments, and tuning the network??  
    e) Need to get this set up and running on the cloud (AWS vs FASRC)

2) Presentation: 
    a) have an outline so far (share outline)
    b) do you want to take a look at the presentation before I give the talk? If so does Monday work? 
    c) any need to share with JQ beforehand? 

3) Future steps: 
    a) is it ok to continue the project (seems like we're closing in on the interesting part + mention paper)?
    b) will RB talk to JQ or should I (preference for RB)? 


9/4/2020 notes: 
1)  Speed-tests:
    a) 150 genes, 6 samples (5T+1V), 25 epochs, local = 54 mins, MSE = 0.15
    b) 150 genes, 6 samples (5T+1V), 25 epochs, AWS cpu = 10 mins, MSE = 0.15
    c) 150 genes, 6 samples (5T+1V), 25 epochs, AWS gpu = 49 mins, MSE = 0.15
seems like CPU on AWS is best option so far..

9/8/2020 notes: 
1) changing neural network #percetrons
    a) will try 2 hidden layers of 300 perceptrons each 
        - (input = 150 genes, output = 150 genes)
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.068
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.064
        - 150 genes, 6 samples (5TAll+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.1
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS cpu = 3.6 hrs, MSE = 0.008
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.008
    b) JQ feedback (lab meeting): 
        - maybe move forward with actual YEAST time-series data (e.g. cell-cycle) first
        - what are these 150 genes in the Yeast GRN (from Bhuva 2019)?

9/9/2020 notes: 
1) Email to Chalmers team
    - Undestand explicit time functionality
        - works with type = single
        - ran on AWS gpu, 30 genes, 6 samples, 3 epochs
    - Understand various batching methods 
    - Understand train/predict based on time = 0 vs all times
2) Check Bhuva 2019 (temporal data)
    - no mention of time-series data (mention found in repository; WE made the changes)
    - they developed the DIFFERENTIAL CO-EXP method using a yeast network, and then applied to TCGA
        - i.e. organism agnostic
    - Marbach D, Schaffter T, Mattiussi C, Floreano D. Generating realistic in silico
gene networks for performance assessment of reverse engineering
methods. J Comput Biol. 2009;16:229â€“39.
    - http://genome-www.stanford.edu/cellcycle/


9/10/2020 notes:
1) Experimenting with explicit time = T and batch_type = single
    - seems to give overall better MSE values when using 30 genes and 6 samples
    - only runs on CPU not GPU (why?)
    - scaling up, let's see what happens:
        - 150 genes, 6 samples (explct = T, batch = single), 
            15 epochs, 300 perceptron-layers, 
            AWS cpu = 2.08 hrs, MSE = 0.007
        - 150 genes, 8 samples (explct = T, batch = single), 
            15 epochs, 300 perceptron-layers, 
            AWS cpu = 1.89 hrs, MSE = 0.01


9/11/2020 notes: 
1) Experimenting with explicit time = T and batch_type = single on t2.2xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = x hrs, MSE = x  
        - (TOOK TOO LONG, no improvement over p2.xlarge)
        - could consider just using this if we only do CPU since it's cheaper
2) Experimenting with explicit time = T and batch_type = single on c5d.4xlarge AWS (CPU only)
    - Instance type supposed to be compute-optimized (16 vcpu + 32GB RAM)
    - 150 genes, 8 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = 1.47 hrs, MSE = 0.007
    - 150 genes, 6 samples (explct = T, batch = single), 
        15 epochs, 300 perceptron-layers, 
        AWS cpu = 1.5 hrs, MSE = 0.005 

9/13/2020 notes:
1) Experimenting with explicit time = T and batch_type = single on c5d.4xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.15), batch_size = 2
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.15), batch_size = 4
2) Experimenting with explicit time = T and batch_type = single on c5.9xlarge AWS (CPU only)
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.2), batch_size = 2
    - 150 genes, 8 samples (explct = T, batch = single, val_split = 0.2), batch_size = 4
3) Have emailed Chalmers team for time to talk about code; waiting...

9/14/202 notes:
1) Investigate explicit time, vis, and batch methods on own
    - batch methods = got a good understanding now; we should probably stick to pairwise train-test (i.e "single")
    - explicit time = from the looks of it, doesn't seem like it affects predictions much (need to confirm)!
    - Now running few experiments to see if effect of explicit time is substantial
    - MSE still around 0.012, but runtime is very fast (<30 mins). 
    - GOOD to keep batch_size = 2 and explicit = False!
    - Let's increase epochs to 25
        - MSE goes down to 0.007
        - So should stick to 25 epochs!
    - Now let's test again on GPU (explicit = F, batch = single)
        - seems like explicit = F, batch = single and batch_size = 2 is optimal 
2) Scope to optimize:
    - good initialization (RB to send)
    - domain knowledge into NN (ask Anuraag about this)
    - graph neural network (GNN)
4) Check for real yeast data
    - yeast/human gene-communities
    - how does it scale? (base how much to scale on the structure of the real yeast data)
    - should we stick to yeast??

9/15/202 notes:
1) Start plotting predicted dynamics
    - made progress in terms of this. got plotting function to work.
    - testing, works fine and plots make sense!
    - tested with 0 noise dataset:
        - looks very good! approximations are very accurate. 

9/16/202 notes:
1) Trying to increase epochs so that training error is essentially 0 on 0 noise dataset.
    - will run on 30 genes, 6 sample, 0 noise dataset for 40 epochs
        - predictions not near-perfect yet. Scaling up epochs and perceptrons  
            - why does training_loss fluctuate, and not monotonically decrease? (Stochastic GD!)
            - think about best_training_model vs best_validation_model
        - changed code so that we also plot training loss over epochs
    - running on 0 noise dataset but with n_val = 0, i.e. val_split = 0
        - we see that we have near-perfect training fit!
    - can use c5.18xlarge as next step ($3.06 per hour), i.e. when scaling up to more genes with 0noise. 

9/17/2020 notes:
1) Meeting with RB. Potential things to try: 
	- larger batch size
	- smaller learning rate decay
	- start with best model already
	- next step scale up to more than 30 genes

9/27/2020 notes:
0) Changed plotting function to plot all 30 genes; also keeping track of training error now
1) Batch_size experiments:
    - When batch_size = 100% full (no stochastic GD), runtime = 4.86 hrs
    - When batch_size = 50%, runtime ~ 3.75
    - When batch_size = 33%, runtime ~ 2.75 hrs
    - When batch_size = 16% , runtime = 2.33 hrs
    - When batch_size = 3% (2) , runtime = 2.24 hrs
    - When batch_size = 1.5% (1; fully stochastic) , runtime = 2.31 hrs
    - In *general*, also saw better performance with smaller batches
2) Things to explore:
    - Performance-wise: different initial learning rates, different network structure, etc:
        -  NeuralODE authors: "Avoid non-smooth non-linearities such as ReLU and LeakyReLU.
            Prefer non-linearities with a theoretically unique adjoint/gradient such as Softplus."
    - Runtime-wise: GPU capacity: 
        - Chalmers authors: "Due to the way the ODE solver was implemented we were only able
         to parallelize computations on the GPU if they were for time steps of the same length.
          Further research should be conducted on the topic of extending the ODE solvers to also
           be able to run different time steps in parallel to speed up training."
        - Need to look more closely into code about how "cude", "cuda:0" are used, and if available
            devices are being used efficiently. 
        - running experiment on AWS p3.2xlarge ($3.02 per hour!):
            - init_lr = 0.001, dec_lr = False, batch size = 54 (for parallelization)
            - MSE = NA, runtime = way too long (40 mins for first epoch!)  

9/28/2020 notes: 
0) Booted a new AWS instance (c5.9xlarge)
1) Question for RB:
    - what is weight-decay? should we use it now? 
2) Experiments with init_lr = 0.001, dec_lr = False:
    - should try out different batch sizes here as well. 

10/04/2020 notes:
0) Booted a new AWS instance (c5.18xlarge)
    - now reporting time and MSE at epochs of interest (25, 40, 50, 80)
1) Trying the non-stochastic (batch_size = 27) upto 160 epochs
    - trying upto 160 epochs with init_lr = 0.005 and 0.001
        - 0.005 better constant lr, but we see that we keep maxing out around ~10^-6
2) Try other things
    - Increased to 500 perceptrons for bs = 27 (non-stochastic)
    - Trying both xavier_uniform and orthogonal initialization schemes
        -orthogonal much better to start with 

10/05/2020 notes:
0) changed file structure to reduce cluttering
1) changed training code:
    - now saving the best performing model instead of final model
    - can load in pre-trained model from .pt file
2) Will now load in best-performing model from before (3E-06), and try to improve using slow lr and decay

10/08/2020 notes:
1) using pre-trained model does not help much
    - tried loading in but gradients seem to go off in unintuitive directions
    - not much change after a large number of epochs
2)  Experimented with:
    - much larger epochs (240) 
    - with fixed lr = 0.05 and with/without a one-time drop if error crosses 5E-5
    - number of perceptrons (50,100, 500)
3) 100 perceptron is feasible, but should go upto 160-200 epochs to ensure ~10^-6 training error
4) Testing 50 perceptron with a one-time lr drop for upto 300 epochs, let's see
    - plateuas off at greater than 10^-5 error rate. 
5) Next thing to do would be to pick an NN size (100 vs 50) and increase the number of genes (still no noise)
    - should probably go with 100 NN and now expanding to genes

10/09/2020 notes:
1) RB suggested doing noise experiment first before expanding to more genes. 
    - check if learner is robust is robust to noise (see MSE w.r.t predicted curve and TRUE mean curve)
    - updated code so that we can introduce noise to no noise data within the program
    - updated plotting code to plot true dynamics (mean function) as well
    - running small experiment:
        - 100 neurons, noise 0.001, fixed lr = 0.005, epochs = 200, c5.9xlarge
        - we see reasonable performance, looks good for proceeding with more noise experiments

10/10/2020 notes: 
1) noise experiments with true_mu are interesting
    - need to discuss with RB the current strategy for evaluating the predicted dynamics w.r.t mu_loss
    - expanding to 8 samples  
    - val_split= 0.25; we have same number of data points as with 6 samples val_split = 0
    - currently testing plots, and basics ... (WORKS!?)
    - now running 8 sample (vs 0.25) experiment for 200 epochs 
    - now running 8 sample (vs 0) experiment for 200 epochs


10/16/2020 notes:
1) After meeting with RB it's clear that our procedure works
    - but need to scale up to include more samples to get better performance
    - also realistic depiction of real-world time-series data (few t, large n, large g)
    - currently scaled up to 50 samples (30 genes)
        - 450 data points (90% training = 405 points = 203/batch, 10% validation = 45 points)
            - good overall performance (true mu MSE down to 4.96E-04)
            - but took 21 hours to run.... 
    - saving MSE_plot more often in case we want to end the experiment early


10/19/2020 notes:
1) Keeping 50 samples (30 genes), but with 50 neurons
    - see if we can get ~4.96E-04 with only 50 neurons, now that we've scaled up samples
    - 450 data points (90% training = 405 points = 203/batch, 10% validation = 45 points)
    - Taking too long, and doesn't seem like we'll cross 1E-03
    - shifting focus away from this for now, to work on larger networks (150 genes)
2) Keeping 50 samples (30 genes), 100 neurons, but on GPU!
    - see if we can see benefits of parallelization, now that we've scaled up samples
    - 450 data points (90% training = 405 points = 203/batch, 10% validation = 45 points)
    - NOT FEASIBLE, VERY SLOW. WILL NEED TO INVESTIGATE LATER:
        - post on Pytorch help community (general neuralODE .backward() question)
        - check if odeint() is the most expensive step in torchdiffeq's backprop algorithm
        - pull new torchdiffeq files from NeuralODE authors' GitHub repo
        - run on GPU with reduced batch-size
3) Keeping 50 samples; 50 neurons; 150 genes (scaled up); times = 0,3,6,9 (scaled down)
    - 150 data points (90% training = 135 points = 68/batch, 10% validation = 15 points)
    - Changed plotting so that only upto 30 genes and upto 7 samples are plotted
    - SCALING UP GENES DOESN'T AFFECT RUNTIME, BUT SCALING DOWN TIME-POINTS DOES!
        - i.e. overall "time-points" is what matters!
    - NOT GETTING GOOD PREDICTIONS:
        - steps seem too shallow. Maybe repeat with larger step-size (init_lr = 0.01)?
        - try upscaling the NN (100 neurons)?
        - try increasing number of samples (150 samples)?
4) Discuss validation strategy with RB 
    - currently doing things in RANDOM pairs instead of ALL pairs per trajectory
    - maybe good to try ALL pairs from 49 people as train, and all pairs from 1 person as test


10/20/2020 notes:
1) Speed-up strategies: 
    - post on Pytorch help community (general neuralODE .backward() question)
    - check if odeint() is the most expensive step in torchdiffeq's backprop algorithm
    - pull new torchdiffeq files from NeuralODE authors' GitHub repo
        - done, seems to run fine! easy integration.
    - run on GPU with reduced batch-size
        - trying now with batch_size = 4
            - GPU = NO BENEFIT, the main benefit is coming from reduced batch size.
        - doesn't look too promising, but our program also does not look properly vectorized
        - can talk to Marouen to see how to properly parallelize it.
    - run on CPU for GPU comparison
        - similar as above, batch_size = 4
        - runtime goes down DUE TO LOWER BATCH SIZE, but performance poor. 
        - MSE is too volatile!
2) Back to realistic dataset(150 samples; 50 neurons; 150 genes; times = 0,3,6,9):
    - 450 data points (90% training = 405 points = 203/batch (2 batches), 10% validation = 45 points)
    - Note that trying 150 samples now, hope is that more data will lead to better prediction (<10^-3)
3) More stuff with realistic dataset (150 samples; 50 neurons; 150 genes; times = 0,3,6,9):
    - Try larger step-size (init_lr = 0.01), lower batchsize (4 batches)
    - see how this compares to analysis above
     


