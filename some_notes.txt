

Questions for Rebekka (early august 2020): 
0) updates:
    a) not too much progress
    b) found resources (thesis that I'm going through)
    c) found various code bases
    d) just to clarify:
        I) parameters are the only things we are learning right?
        II) we PROVIDE the functional form of the ODE to the algo?  
   
1) what can neural ODEs achieve:
    a) need the underlying mouse gene network
    b) can only get parameters
    c) or maybe functional form of ODE, but not the nature of the network

2) plans for upcoming 2 weeks: 
    a) no fixed code base
    b) any pytorch resources?
    c) any go-to person in the lab for queries/updates? 
    d) any need to write up anything?


August 21 2020: 
0) Theoritical discussion: 
    a) no functional forms go into NeuralODE algorithm.
    b) two parts, NN + ODESolver, where the NN *is* the derivative functional
    c) Can do functional fitting to retrieve parameters
    
1) Updates: 
    a) a lot of progress made, have a solid pipeline set-up and currently producing results
    b) Sweden story; + IH modifications (pytorch resources) 
    c) The code base can do a lot, various forms of sampling as well as functional fitting, but some parts are confusing
    d) Parameters, and results obtained so far
    e) Need to send an email to Sweden regarding various questions; any IP issues/ anything to be wary of? 
    f) After getting clarification from them, then perhaps can start designing experiments, and tuning the network??  
    e) Need to get this set up and running on the cloud (AWS vs FASRC)

2) Presentation: 
    a) have an outline so far (share outline)
    b) do you want to take a look at the presentation before I give the talk? If so does Monday work? 
    c) any need to share with JQ beforehand? 

3) Future steps: 
    a) is it ok to continue the project (seems like we're closing in on the interesting part + mention paper)?
    b) will RB talk to JQ or should I (preference for RB)? 


9/4/2020 notes: 
1)  Speed-tests:
    a) 150 genes, 6 samples (5T+1V), 25 epochs, local = 54 mins
    b) 150 genes, 6 samples (5T+1V), 25 epochs, AWS cpu = 10 mins
    c) 150 genes, 6 samples (5T+1V), 25 epochs, AWS gpu = 49 mins
seems like CPU on AWS is best option so far..

9/8/2020 notes: 
1) changing neural network #percetrons
    a) will try 2 hidden layers of 300 perceptrons each 
        - (input = 150 genes, output = 150 genes)
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.068
        - 150 genes, 6 samples (5TFrom0+1VFrom0), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.064
        - 150 genes, 6 samples (5TAll+1VFrom0), 25 epochs, AWS cpu = 1.7 hrs, MSE = 0.1
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS cpu = 3.6 hrs, MSE = 0.008
        - 150 genes, 6 samples (5TAll+1VAll), 25 epochs, AWS gpu = 2.3 hrs, MSE = 0.008
    b) JQ feedback (lab meeting): 
        - maybe move forward with actual YEAST time-series data (e.g. cell-cycle) first
        - what are these 150 genes in the Yeast GRN (from Bhuva 2019)?

9/9/2020 notes: 
1) Email to Chalmers team
    - Undestand explicit time functionality
        - works with type = single
    - Understand various batching methods 
    - Understand train/predict based on time = 0 vs all times
2) Check Bhuva 2019 (temporal data)
3) Run experiment with 150 genes but more samples (8TAll+1VAll)
4) Take good model(s) and make trajectory comparison plots